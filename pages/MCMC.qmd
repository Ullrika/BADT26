---
title: "MCMC-sampling"
subtitle: "BADT"
author: "Zheng Zhou"
---

## Learning goals

(@) describe what is a Markov chain monte carlo (MCMC)

(@) describe the purpose and applicability range of MCMC

(@) compare the strength and weakness of MCMC with other posterior estimation methods

(@) develop a simple MCMC (complete exercise 3)

(@) describe MCMC diagnostics

## Beta-binomial conjugation

Binomial likelihood:

n Bernoulli trials with y successes and unknown probability of success $\theta$.

$$
 \text{Binomial}(n, y,\theta)
$$

The prior is a beta distribution:

$$
 Beta(\alpha, \beta)
$$

Conjugation: the posterior distribution is also a beta distribution:

$$
  Beta(\alpha + y, \beta + n - y)
$$

## Posterior computation

1. Analytical solution: conjugation often impossible

2. Grid approximation: possible but efficiency drops exponentialy with dimension

3. LaPlace approximation: assumption of multivariate normality often not applicable

4. **Markov Chain Monte Carlo (MCMC)**: flexible, efficient

## What is MCMC

* Markov chain
  * Markov process: $f(t+1)=g(f(t),noise)$
  * Chain
  
* Monte Carlo: a large number of samples can approximate the distribution

MCMC transforms the question:

**FROM**:

Getting the posterior distribution

**TO**

Getting enough representative samples from the posterior distribution

## Example: Island hopping

1. Make a proposal.

2. Acceptance ratio $p_{prop}=\frac{poposal}{current}$.

3. If $p_{prop}$ > 1, move; if $p_{prop}$=1, stay.

4. If $p_{prop}$ < 1, generate a random value k between (0,1). If $k<p_{prop}$, then move.

5. repeat 1-4.

### Simulate the hopping process

Number of islands (parameter values) unknown in reality.

```{r, islands}
#| warning: false
#| message: false
#| echo: false
library(tidyverse)

num_island = 5

island_size = tibble(
  island = 1:num_island,
  Size = 1:num_island
)
p0 = ggplot(island_size,aes(x=island,y=Size))+
  geom_col()+
  theme_classic()+
  scale_y_continuous(limits = c(0,(num_island+1)))+
  scale_x_continuous(breaks=1:num_island)+
  labs(title = "Island size",
       x = "Island number",
       y = "Size")
p0
```

---

```{r islands_function,echo=T,include=T}
# Function to implement the Metropolis algorithm
target_distribution <- sapply(1:num_island,function(x){
    prob = x/sum(1:num_island)
  })

metropolis_island_hopping <- function(num_iterations, starting_island = round(num_island/2,0),true_prob = target_distribution) {
  # Number of islands
  num_islands <- num_island
  
  # Define transition probabilities (in this case, uniform)
  # This means proposals are equally likely to jump to adjacent islands
  
  # Initialize chain
  chain <- numeric(num_iterations)
  chain[1] <- starting_island
  
  # Define stationary distribution (target)
  # In this example, we'll use a uniform distribution
  target_distribution <- true_prob
  
  # Run Metropolis algorithm
  for (i in 2:num_iterations) {
    current <- chain[i-1]
    
    # Propose a move: randomly jump to an adjacent island or stay
    # For simplicity, adjacency is defined as +/- 1 
    jump <- sample(c(-1, 0, 1), 1)
    proposed <- current + jump
    
    # Check boundaries and wrap around (circular islands)
    if (proposed < 1) {
      proposed <- num_islands + proposed
    } else if (proposed > num_islands) {
      proposed <- proposed - num_islands
    }
    
    # Calculate acceptance probability
    acceptance_ratio <- target_distribution[proposed] / target_distribution[current]
    acceptance_prob <- min(1, acceptance_ratio)
    
    # Accept or reject the proposal
    if (runif(1) < acceptance_prob) {
      chain[i] <- proposed  # Accept move
    } else {
      chain[i] <- current   # Reject move, stay at current island
    }
  }
  
  return(chain)
}



```

---

```{r island_run,include=T,echo=T}
n_iter = 1000
chain = metropolis_island_hopping(num_iterations = n_iter)

hopping_freq = table(chain) / n_iter
hopping_prob = numeric(num_island)
for (i in 1:num_island) {
  hopping_prob[i] <- sum(chain == i) / n_iter
}
hopping_results = tibble(
  islands = 1:num_island,
  freq = hopping_prob
)
hopping_results
```

---

```{r metropolis_plot,include=T,echo=T}
trace_data <- data.frame(
  Iteration = 1:n_iter,
  Island = chain[1:n_iter]
)
p2 <- ggplot(trace_data, aes(x = Iteration, y = Island)) +
  geom_line() +
  geom_point(size = 1) +
  labs(title = "Trace Plot",
       x = "Iteration",
       y = "Island Number") +
  scale_y_continuous(breaks = 1:num_island) +
  theme_minimal()
p2
```

### Summary of hopping

When repeated enough times, frequency on any island matches the relative population of the island.

Three critical things:

1. known your position and adjacent options

2. making a proposal and knowing the population of the proposal

3. knowing the population of the current, to calculate the acceptance ratio $p_{prop}=\frac{proposal}{current}$.


## Metropolis algorithm

![](img/Metropolis_paper.bmp)

Island hopping is a special case, Metropolis algorithm could handle:

* continuous positions

* more than one dimension

* complex proposals

$$
y=\beta_0 + \beta_1 x + \epsilon
$$

Model:

$$
\begin{equation}
\begin{split}
\eta \sim N(0,\sigma^2) \\
y \sim N(\beta_0 + \beta_1 x, \epsilon)
\end{split}
\end{equation}
$$

$\beta_0, \beta_1$ are continuous parameters

Likelihood:

$$
p(Y|\beta_0,\beta_1,\epsilon,x)=\prod_i^{n} \frac{1}{\sqrt{2\pi \sigma^2}}exp(-\frac{(y-(\beta_0+\beta_1 x))^2}{2\sigma^2})
$$

Priors:

$\beta_0 \sim N(0,5)$.

How does the island hopping analogy work here?

## Exercise 3

Complete Ex 3: today and beginning of tomorrow's session. 

## MCMC diagnostics

Before accepting MCMC results, how good they are?

* Trace plot: displays chain stability in random sampling, 

* Rhat/ Gelman-Rubins statistics: convergence of multiple chains
  
* Effective sample size: measures chain accuracy- how many independent sampling steps for the chain to approximate the posterior

* Autocorrelation: also shows chain accuracy- strong correlation between steps that are one or two steps apart is bad.

Read the Bayes rule book chapter 6.3  and DBDA chapter 7.5 for technical details and examples.


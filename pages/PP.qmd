---
title: "Predictive Performance"
author: "Zheng Zhou"
format:
  html: default
---


## Recap of MCMC

| Objective | Desired property | Analytical solution | Approximation without analytical solution |
|:---:|:---|:---|:---|
| Model aggregation | Mean and variance of the aggregated variable | Analytical solution using Tyler expansion and Lagrangian transformation | Monte Carlo simulation |
| Posterior sampling | Posterior distribution | Conjugated posterior | MCMC |

Table: MCMC approximates samples from the posterior distribution without analytical solutions

## Second-half learning goals

1. Describe how prior distributions and observed data influence posterior distributions
  - Apply this understanding to interpret posterior predictions across different scenarios with varying priors (flat, informed) and data (sparse, abundant)
  - Reflect on the ethical and practical implication of prior choices in modeling

2. Describe the purpose of evaluating predictive performance
  - Describe why evaluation of predictive performance is necessary for model validation, generalization and avoiding overfitting
  - Describe the difference between in-sample and out-of-sample evaluation of predictive performance
  - Reflect on the limitations of relying solely on predictive performance for decision making
  
3. Describe categories of predictive performance metrics
  - Describe the key difference between the categories
  - Describe Bayesian predictive measures

## Impact on posterior from prior and data

```{r post,message=F,warning=F,echo=F}
library(tidyverse)

# Function to calculate Beta posterior parameters after observing binomial data
calculate_posterior <- function(prior_a, prior_b, successes, trials) {
  post_a <- prior_a + successes
  post_b <- prior_b + trials - successes
  return(list(a = post_a, b = post_b))
}

# Creating a structured design with:
# - Columns: Flat prior, weak prior, strong prior
# - Rows: Different data scenarios (varying success rates and sample sizes)

# Define the matrix of scenarios
scenarios <- expand.grid(
  prior_strength = c("Flat", "Weak", "Strong"),
  data_scenario = c("Low Success Rate", "Medium Success Rate", "High Success Rate")
)

# Assign specific parameter values based on the scenario
scenarios$prior_a <- 1
scenarios$prior_b <- 1

# Modify prior parameters based on prior strength
# Using Beta(1,1) for flat, Beta(2,2) for weak, Beta(10,10) for strong
scenarios$prior_a[scenarios$prior_strength == "Weak"] <- 2
scenarios$prior_b[scenarios$prior_strength == "Weak"] <- 2
scenarios$prior_a[scenarios$prior_strength == "Strong"] <- 10
scenarios$prior_b[scenarios$prior_strength == "Strong"] <- 10

# Set data parameters (successes, trials) based on data scenario
# Each row will have different success rate AND sample size
# Row 1: Low success rate (20%) with small sample size
# Row 2: Medium success rate (50%) with moderate sample size
# Row 3: High success rate (80%) with large sample size

scenarios$trials <- 10      # Small sample size
scenarios$successes <- 2    # 20% success rate

# Adjust trials and successes based on data scenario
scenarios$trials[scenarios$data_scenario == "Medium Success Rate"] <- 30    # Moderate sample size
scenarios$successes[scenarios$data_scenario == "Medium Success Rate"] <- 15 # 50% success rate

scenarios$trials[scenarios$data_scenario == "High Success Rate"] <- 100    # Large sample size
scenarios$successes[scenarios$data_scenario == "High Success Rate"] <- 80  # 80% success rate

# Add row and column IDs for faceting
scenarios$row_id <- as.numeric(factor(scenarios$data_scenario, 
                                      levels = c("Low Success Rate", "Medium Success Rate", "High Success Rate")))
scenarios$col_id <- as.numeric(factor(scenarios$prior_strength, 
                                      levels = c("Flat", "Weak", "Strong")))

# Function to prepare data for a single scenario WITH FIXED SCALING FOR LIKELIHOOD
prepare_scenario_data <- function(prior_a, prior_b, successes, trials) {
  # Calculate posterior parameters
  posterior <- calculate_posterior(prior_a, prior_b, successes, trials)
  
  # Create sequence for x-axis
  p_seq <- seq(0, 1, length.out = 500)
  
  # Calculate densities
  prior_density <- dbeta(p_seq, prior_a, prior_b)
  posterior_density <- dbeta(p_seq, posterior$a, posterior$b)
  
  # For likelihood, we use scaled binomial PMF
  likelihood <- dbinom(successes, size = trials, prob = p_seq)
  
  # Instead of scaling based on max of prior/posterior, use a FIXED SCALING FACTOR
  # This ensures likelihood is visible in all cells
  max_like <- max(likelihood)
  
  # Use a consistent scaling approach that works across all cells
  # Scale likelihood to have same height as 1/3 of the max of prior and posterior
  scaling_target <- max(c(max(prior_density), max(posterior_density))) * 0.5
  likelihood_scaled <- likelihood / max_like * scaling_target
  
  # Create and return data frame
  df <- data.frame(
    p = rep(p_seq, 3),
    density = c(prior_density, likelihood_scaled, posterior_density),
    distribution = rep(c("Prior", "Likelihood", "Posterior"), each = length(p_seq)),
    prior_params = rep(paste0("Beta(", prior_a, ",", prior_b, ")"), 3 * length(p_seq)),
    data_summary = rep(paste0(successes, "/", trials), 3 * length(p_seq))
  )
  
  return(df)
}

# Create a combined data frame for all scenarios
all_scenarios_data <- data.frame()

for (i in 1:nrow(scenarios)) {
  scenario_df <- prepare_scenario_data(
    scenarios$prior_a[i],
    scenarios$prior_b[i],
    scenarios$successes[i],
    scenarios$trials[i]
  )
  
  # Add scenario information
  scenario_df$prior_strength <- scenarios$prior_strength[i]
  scenario_df$data_scenario <- scenarios$data_scenario[i]
  scenario_df$row_id <- scenarios$row_id[i]
  scenario_df$col_id <- scenarios$col_id[i]
  
  # Bind to the combined data frame
  all_scenarios_data <- rbind(all_scenarios_data, scenario_df)
}

# Calculate posterior parameters for each scenario
scenarios <- scenarios %>%
  rowwise() %>%
  mutate(
    posterior_a = prior_a + successes,
    posterior_b = prior_b + trials - successes,
    success_rate = round(100 * successes / trials)
  )

# Create faceted plot
facet_plot <- ggplot(all_scenarios_data, 
                    aes(x = p, y = density, color = distribution, linetype = distribution)) +
  geom_line(size = 1.1) +
  facet_grid(row_id ~ col_id, 
             labeller = labeller(
               row_id = c(
                 "1" = "Low Success Rate - 2/10 (20%) Small Sample",
                 "2" = "Medium Success Rate - 15/30 (50%) Medium Sample",
                 "3" = "High Success Rate - 80/100 (80%) Large Sample"
               ),
               col_id = c(
                 "1" = "Flat Prior: Beta(1,1)",
                 "2" = "Weak Prior: Beta(2,2)",
                 "3" = "Strong Prior: Beta(10,10)"
               )
             )) +
  scale_color_manual(values = c("Prior" = "blue", "Likelihood" = "darkgreen", "Posterior" = "red")) +
  scale_linetype_manual(values = c("Prior" = "dashed", "Likelihood" = "dotted", "Posterior" = "solid")) +
  labs(
    title = "Effects of Prior Strength and Data Characteristics on Posterior Distributions",
    subtitle = "Each row shows different data scenarios with varying success rates and sample sizes",
    x = "Probability (p)",
    y = "Density"
  ) +
  theme_minimal() +
  theme(
    legend.title = element_blank(),
    legend.position = "bottom",
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 10),
    strip.text = element_text(size = 9, face = "bold"),
    strip.background = element_rect(fill = "lightgray", color = NA),
    panel.spacing = unit(1, "lines"),
    panel.border = element_rect(color = "gray", fill = NA),
    panel.grid.minor = element_blank()
  )

# Create a more informative subtitle layer
subtitle_data <- scenarios %>%
  mutate(
    label = paste0(
      "Prior: Beta(", prior_a, ",", prior_b, ") | ",
      "Data: ", successes, "/", trials, " (", success_rate, "%)\n",
      "Posterior: Beta(", posterior_a, ",", posterior_b, ")"
    ),
    x = 0.5,
    y = Inf,
    hjust = 0.5,
    vjust = 2
  )

# Add subtitle to each facet
facet_plot_with_labels <- facet_plot +
  geom_text(
    data = subtitle_data,
    aes(x = x, y = y, label = label, hjust = hjust, vjust = vjust),
    inherit.aes = FALSE,
    size = 2.5
  )

# Display the facet plot
print(facet_plot_with_labels)

```

---

Discussion questions (10-min):

1. Look at each column. Which lines are different across rows? Why?

2. Look at each row. Which lines are different across columns? Why?

3. We cannot change data. But we can change prior. In your opinion, is it better to use flat prior, weak or strong prior?

4. What could go wrong with prior manipulation?

## The ratione to evaluate predictive performance

Playing with models is fun. Which one is better?

We live in a real world with limited resources. How do we know our model is good enough to be used in the real world?

* Avoid overfitting
* Align with practical goals
* Ensure generalization with context

Scope of evaluation: in-sample vs out-of-sample

## Overfitting

![Best example of overfitting](./overfitting.jpeg)

## Generalization with practical context

A model to predict gelato consumption based on swimming activity.

If the model is trained on survey data by people in Lund in summer, will the predictions apply to people in MalmÃ¶ winter?

Will the predictions apply to people in Qatar (in desert) in summer?

## Predictive performance vs goodness-of-fit

Goodness of fit: how well the model describes the observed data?

Predictive performance: how well the model predicts new data? 
* Goodness of fit is a component of predictive performance

## Category of predictive performance metrics

| Category | Description | Example |
|:---:|:---|:---|
| Central tendency | Considers **ONLY** the mean distance between observed and predictive values | mean rooted squared errors |
| Incomplete uncertainty | In addition to central tendency, also considers **uncertainty** from the spread (variance) | Ranked Probability Score |
| Complete uncertainty | Full predictive _density_ distribution | Predictive density |

## Predictive performance metrics for Bayesian models

Bayesian models have a full posterior distribution already!

With sufficient computing power, no reason not to use full predictive distribution.

Examples for those interested: Watanabe-Akaike Information criteria (WAIC), expected log predictive density (elpd)...

## Out-of-sample evaluation

Cross-validation

![K-fold cross validation](./k-CV.jpg)

True remedy to out-of-sample prediction problem is to improve the representativeness of sampling protocol.
  - Not necessarily related to sample size
  - Be aware of bias in sampling protocol and reduce the bias
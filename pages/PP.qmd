---
title: "Predictive Performance"
author: "Zheng Zhou"
format:
  html: default
---


## Recap of MCMC

| Objective | Desired property | Analytical solution | Approximation without analytical solution |
|:---:|:---|:---|:---|
| Model aggregation | Mean and variance of the aggregated variable | Analytical solution using Tyler expansion and Lagrangian transformation | Monte Carlo simulation |
| Posterior sampling | Posterior distribution | Conjugated posterior | MCMC |

Table: MCMC approximates samples from the posterior distribution without analytical solutions

## Second-half learning goals

1. Describe how prior distributions and observed data influence posterior distributions
  - Apply this understanding to interpret posterior predictions across different scenarios with varying priors (flat, informed) and data (sparse, abundant)
  - Reflect on the ethical and practical implication of prior choices in modeling

2. Describe the purpose of evaluating predictive performance
  - Describe why evaluation of predictive performance is necessary for model validation, generalization and avoiding overfitting
  - Describe the difference between in-sample and out-of-sample evaluation of predictive performance
  - Reflect on the limitations of relying solely on predictive performance for decision making
  
3. Describe categories of predictive performance metrics
  - Describe the key difference between the categories
  - Describe Bayesian predictive measures


---

Discussion questions (10-min):

1. Look at each column. Which lines are different across rows? Why?

2. Look at each row. Which lines are different across columns? Why?

3. We cannot change data. But we can change prior. In your opinion, is it better to use flat prior, weak or strong prior?

4. What could go wrong with prior manipulation?

## The ratione to evaluate predictive performance

Playing with models is fun. Which one is better?

We live in a real world with limited resources. How do we know our model is good enough to be used in the real world?

* Avoid overfitting
* Align with practical goals
* Ensure generalization with context

Scope of evaluation: in-sample vs out-of-sample

## Overfitting

![Best example of overfitting](./overfitting.jpeg)

## Generalization with practical context

A model to predict gelato consumption based on swimming activity.

If the model is trained on survey data by people in Lund in summer, will the predictions apply to people in Malm√∂ winter?

Will the predictions apply to people in Qatar (in desert) in summer?

## Predictive performance vs goodness-of-fit

Goodness of fit: how well the model describes the observed data?

Predictive performance: how well the model predicts new data? 
* Goodness of fit is a component of predictive performance

## Category of predictive performance metrics

| Category | Description | Example |
|:---:|:---|:---|
| Central tendency | Considers **ONLY** the mean distance between observed and predictive values | mean rooted squared errors |
| Incomplete uncertainty | In addition to central tendency, also considers **uncertainty** from the spread (variance) | Ranked Probability Score |
| Complete uncertainty | Full predictive _density_ distribution | Predictive density |

## Predictive performance metrics for Bayesian models

Bayesian models have a full posterior distribution already!

With sufficient computing power, no reason not to use full predictive distribution.

Examples for those interested: Watanabe-Akaike Information criteria (WAIC), expected log predictive density (elpd)...

## Out-of-sample evaluation

Cross-validation

![K-fold cross validation](./k-CV.jpg)

True remedy to out-of-sample prediction problem is to improve the representativeness of sampling protocol.
  - Not necessarily related to sample size
  - Be aware of bias in sampling protocol and reduce the bias
